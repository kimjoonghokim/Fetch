# Answer Confidence Scoring System

This directory contains a comprehensive scoring system for evaluating the confidence of answers generated by Large Language Models (LLMs).

## 📁 Files Overview

- **`scoring.py`** - Main scoring system with confidence and vote calculation methods
- **`demo.py`** - Interactive demonstration and testing script with vote scoring examples

## 🎯 What is Confidence Scoring?

When an AI model generates text, it doesn't just pick one option - it considers many possible words/tokens and assigns probabilities to each. **Confidence scoring** uses these probabilities to determine how "sure" the model is about its answer.

Think of it like asking someone a math question:
- High confidence: "2+2=4" (very sure)
- Low confidence: "What's the 47th digit of pi? Um... maybe 3?" (uncertain)

## 🧮 Confidence Scoring Methods Explained

### 1. **Average Confidence** (Recommended)
- **What it is**: The average confidence across all tokens in the answer
- **Scale**: 0.0 to 1.0 (higher = more confident)
- **Use when**: You want a balanced overall confidence score
- **Example**: If the model generates "The answer is 4" with token confidences [0.9, 0.8, 0.7, 0.9], the average is 0.825

### 2. **Minimum Confidence** (Conservative)
- **What it is**: The confidence of the least confident token
- **Scale**: 0.0 to 1.0 (higher = more confident)
- **Use when**: You want to identify weak spots in the answer
- **Example**: Same answer as above, minimum confidence would be 0.7 (the weakest link)

### 3. **Geometric Confidence** (Multiplicative)
- **What it is**: Geometric mean of all token confidences
- **Scale**: 0.0 to 1.0 (higher = more confident)
- **Use when**: You want to penalize low-confidence tokens more heavily
- **Example**: More sensitive to outliers than average confidence

### 4. **Perplexity** (Technical)
- **What it is**: A measure of how "surprised" the model is by its own answer
- **Scale**: 1.0+ (lower = more confident)
- **Use when**: You're doing technical analysis or comparing across different text lengths
- **Example**: Perplexity of 1.2 (good) vs 5.8 (poor)

## 🏆 Overall Scoring Methods

The scoring system provides three different ways to get an overall score that combines multiple factors (currently just confidence, but designed for future expansion):

### 1. `get_overall_score()` - 📊 Detailed Analysis
- **Type**: Class method (requires `AnswerScorer` instance)
- **Returns**: Large dictionary with complete breakdown
- **Best for**: Debugging, analysis, understanding what's happening
- **Performance**: Slower (more data processing)

```python
scorer = AnswerScorer()
result = scorer.get_overall_score("What is 2+2?", "")

# Returns detailed dictionary:
{
    'overall_score': 0.756,
    'component_scores': {
        'confidence': {'score': 0.756, 'details': {...}},
        'parent_child_quality': {'score': 0.0, 'details': {...}},
        # ... more components
    },
    'weights_used': {'confidence': 1.0},
    'total_weight': 1.0,
    'metadata': {...}
}
```

### 2. `get_simple_overall_score()` - 🔢 Just the Number  
- **Type**: Class method (requires `AnswerScorer` instance)
- **Returns**: Single float (0-1)
- **Best for**: When you already have a scorer instance but just need the final score
- **Performance**: Fast

```python
scorer = AnswerScorer()
score = scorer.get_simple_overall_score("What is 2+2?", "")
# Returns: 0.756
```

### 3. `get_overall_answer_score()` - ⚡ One-Liner
- **Type**: Standalone function (no class needed)
- **Returns**: Single float (0-1)
- **Best for**: Search algorithms, quick scoring, when you don't want to create instances
- **Performance**: Fast

```python
from scorer.scoring import get_overall_answer_score

score = get_overall_answer_score("What is 2+2?", "")
# Returns: 0.756
```

### 🎯 Which One Should You Use?

| Use Case | Recommended Method | Why? |
|----------|-------------------|------|
| **Search Algorithms** | `get_overall_answer_score()` | Simple one-liner, no instance needed |
| **Tree Search (Beam, MCTS)** | `get_overall_answer_score()` | Fast, clean integration |
| **Debugging/Analysis** | `get_overall_score()` | See all component scores and details |
| **Batch Processing** | `get_overall_answer_score()` | Stateless, easy to parallelize |
| **Research/Tuning** | `get_overall_score()` | Full visibility into scoring components |

### 🔗 Method Relationship
```
get_overall_answer_score()  
    ↓ creates AnswerScorer instance
    ↓ calls get_simple_overall_score()
        ↓ calls get_overall_score()
        ↓ extracts just the 'overall_score' field
```

## 🚀 Quick Start

### Prerequisites
Make sure your vLLM policy server is running:
```bash
python3 -m vllm.entrypoints.openai.api_server \
  --model xmu-nlp/Llama-3-8b-gsm8k \
  --port 8000 \
  --dtype float16 \
  --tensor-parallel-size 2 \
  --swap-space 8 \
  --max-model-len 4096
```

### Run the Demo
```bash
cd /workspace/Fetch
python scorer/demo.py
```

This will show you:
- ✅ Basic confidence scoring examples
- 📊 Detailed scoring with all metrics
- 🪜 Step-by-step reasoning confidence
- 🔄 Comparison of different scoring methods
- 🏆 Overall scoring demonstrations

## 💻 Usage Examples

### For Search Algorithms (Recommended)
```python
from scorer.scoring import get_overall_answer_score

# In your beam search, MCTS, etc.
score = get_overall_answer_score("What is 2+2?", "")
print(f"Score: {score:.3f}")  # Score: 0.756

# With custom weights (when more components are available)
weights = {"confidence": 0.8, "length_penalty": 0.2}
score = get_overall_answer_score("What is 2+2?", "", weights=weights)
```

### For Analysis and Debugging
```python
from scorer.scoring import AnswerScorer

scorer = AnswerScorer()
result = scorer.get_overall_score("What is 15*23?", "")

print(f"Overall Score: {result['overall_score']:.3f}")
print(f"Confidence: {result['component_scores']['confidence']['score']:.3f}")
print(f"Generated Text: {result['component_scores']['confidence']['details']['text_generated']}")
```

### Simple Confidence Check
```python
from scorer.scoring import get_simple_confidence

# Get just confidence score (0-1)
confidence = get_simple_confidence("What is 2+2?")
print(f"Confidence: {confidence:.3f}")
```

### Step-by-Step Reasoning
```python
# Evaluate confidence with previous context
question = "What is 25*16?"
context = "I need to multiply 25 by 16. Let me break this down:"

score = get_overall_answer_score(question, context)
print(f"With context score: {score:.3f}")
```

### Batch Processing
```python
questions = ["What is 1+1?", "What is 5*7?", "What is 100/4?"]
scores = [get_overall_answer_score(q, "") for q in questions]
print(f"Scores: {[f'{s:.3f}' for s in scores]}")
```

## 🔧 Configuration

You can customize the scoring system:

```python
from scorer.scoring import PolicyConfig, AnswerScorer

# Custom configuration
config = PolicyConfig(
    url="http://127.0.0.1",
    port=8000,
    model_name="your-model-name",
    temperature=0.5,
    max_tokens=256,
    # ESM service configuration
    esm_url="http://127.0.0.1",
    esm_port=8003
)

scorer = AnswerScorer(config)
```

### ESM Service Setup

To use the full vote scoring capabilities, you need the ESM service running:

1. **Start ESM Service**:
   ```bash
   cd cluster/
   python server_cluster.py
   ```

2. **Check ESM Status**:
   ```python
   from scorer.scoring import check_esm_service_status
   
   if check_esm_service_status():
       print("ESM service is available")
   else:
       print("ESM service is unavailable - using fallback clustering")
   ```

3. **Custom ESM Endpoint**:
   ```python
   config = PolicyConfig(
       esm_url="http://your-esm-server.com",
       esm_port=8003
   )
   ```

## 📊 Understanding Results

### Good Overall Scores
- **Score > 0.7**: Model is quite confident overall
- **Score > 0.5**: Acceptable confidence  
- **Score > 0.3**: Lower confidence but potentially usable

### Poor Overall Scores
- **Score < 0.3**: Model is uncertain
- **Score < 0.1**: Very low confidence, likely unreliable

### Example Output
```
📝 Question: What is 12*15?
🏆 Overall Score: 0.756

📊 Component Breakdown:
   • confidence: 0.756
   • length_penalty: 1.000

🔤 Token Details:
   'The' → 0.834
   ' answer' → 0.756
   ' is' → 0.623
   ' 180' → 0.812
   '.' → 0.891
```

## 🔍 Troubleshooting

### "Failed to parse" URL Error
**Problem**: URL shows as `http://127.0.0.1:8000:8000/v1/completions`
**Solution**: In `scoring.py`, line 38, change:
```python
# From:
url: str = "http://127.0.0.1:8000"
# To:
url: str = "http://127.0.0.1"
```

### "Cannot connect to server"
**Problem**: Demo shows connection error
**Solutions**:
1. Make sure vLLM server is running (see Prerequisites above)
2. Check if port 8000 is available
3. Verify the model name matches your server configuration

### "No confidence scores returned"
**Problem**: All confidence values are None or 0.0
**Solutions**:
1. Check that `logprobs=5` is being sent in the API request
2. Verify your vLLM server supports logprobs
3. Check server logs for errors

## 🧪 Testing Your Setup

Quick test to verify everything works:

```bash
cd /workspace/Fetch
python3 -c "
from scorer.scoring import get_overall_answer_score
try:
    score = get_overall_answer_score('What is 1+1?')
    print(f'✅ Success! Overall Score: {score:.3f}')
except Exception as e:
    print(f'❌ Error: {e}')
"
```

## 🔮 Future Features

The scoring system is designed to be extensible. Planned features include:

- **Parent/Child Node Quality**: Score based on reasoning tree structure
- **Semantic Similarity**: Compare answers to reference solutions
- **Coherence Scoring**: Evaluate logical flow and consistency
- **Factual Consistency**: Check against knowledge bases
- **Length Penalty**: Already implemented, can be enabled with weights
- **Custom Scoring Functions**: Add your own confidence metrics

## 📚 Integration Examples

### Beam Search Integration
```python
from scorer.scoring import get_overall_answer_score

def evaluate_beam_candidates(question, candidates):
    scored_candidates = []
    for candidate_path in candidates:
        score = get_overall_answer_score(question, candidate_path)
        scored_candidates.append((candidate_path, score))
    return sorted(scored_candidates, key=lambda x: x[1], reverse=True)
```

### MCTS Integration
```python
from scorer.scoring import get_overall_answer_score

class MCTSNode:
    def __init__(self, question, path):
        self.question = question
        self.path = path
        self.confidence_score = get_overall_answer_score(question, path)
    
    def get_value(self):
        # Combine verifier score with confidence
        return self.verifier_value * 0.7 + self.confidence_score * 0.3
```

## 🗳️ Vote Scoring (NEW!)

**Vote scoring** is a new feature that rewards answers based on how many semantically similar paths have been found during search. Think of it like a voting system where similar approaches get grouped together and receive higher scores.

### How Vote Scoring Works

1. **Semantic Clustering**: The system groups similar answer paths using the ESM (Embedding-based Semantic Model) service
2. **Merge Counting**: Each group of similar paths represents a potential "merge" operation
3. **Score Calculation**: Higher merge counts lead to higher vote scores

### ESM Service Integration

The vote scoring system integrates with the ESM service for high-quality semantic similarity:

- **Primary Method**: Uses ESM service at `http://127.0.0.1:8003/predict` for semantic clustering
- **Fallback Method**: Falls back to simple word overlap if ESM service is unavailable
- **Configuration**: ESM endpoint can be customized via `PolicyConfig.esm_url` and `PolicyConfig.esm_port`

### Vote Score Calculation

- **Base Score**: 1.0 (every answer starts here)
- **Merge Bonus**: +0.1 per potential merge opportunity
- **Size Bonus**: +0.05 per additional similar path in the current cluster
- **Maximum**: Capped at 2.0 to prevent runaway scoring

### Example

```python
from scorer.scoring import get_vote_score_simple

# With semantically similar candidates
candidates = [
    "Let me solve this step by step",
    "I'll calculate this multiplication", 
    "Let me work through this problem"
]

vote_score = get_vote_score_simple("What is 15*8?", "Let me solve this", candidates)
# Result: ~1.3 (indicating 3 similar approaches found)
```

### When to Use Vote Scoring

- **Search Algorithms**: When you have multiple candidate paths to compare
- **Ensemble Methods**: To reward answers that multiple approaches agree on
- **Quality Assessment**: To identify answers that are "popular" among similar approaches

---

*Need help? Check the demo output or create an issue with your specific error message.*

## 🔍 Integration with Search Algorithms

The vote scoring system is designed to integrate seamlessly with search algorithms that perform semantic similarity merging. Here's how to use it:

### Beam Search Integration

```python
def score_beam_candidates(question, current_path, candidate_nodes):
    """Score candidates in beam search using combined scoring."""
    candidate_paths = [node.content for node in candidate_nodes]
    
    # Get combined score (confidence + vote)
    score = get_overall_answer_score(
        question, 
        current_path, 
        candidate_paths=candidate_paths
    )
    return score

# In your beam search loop:
for candidate in beam_candidates:
    score = score_beam_candidates(question, current_path, [candidate])
    # Use score for beam selection
```

### MCTS Integration

```python
def evaluate_mcts_node(question, node_path, sibling_paths):
    """Evaluate MCTS node using vote scoring."""
    # Get vote score based on sibling similarity
    vote_score = get_vote_score_simple(question, node_path, sibling_paths)
    
    # Combine with other node evaluation metrics
    return vote_score * 0.3 + other_metrics * 0.7
```

## 🚨 Troubleshooting

### ESM Service Issues

**Problem**: Vote scoring falls back to simple clustering
**Solution**: 
1. Check if ESM service is running: `python cluster/server_cluster.py`
2. Verify endpoint: `http://127.0.0.1:8003/predict`
3. Check firewall/network settings

**Problem**: ESM service timeout errors
**Solution**:
1. Increase timeout in configuration
2. Check ESM service performance
3. Reduce batch size for clustering

**Problem**: Different clustering results than expected
**Solution**:
1. Adjust similarity threshold (default: 0.15)
2. Check ESM model quality
3. Verify text preprocessing

### Vote Scoring with Candidate Paths
```python
# Evaluate answer considering similar candidate paths
question = "What is 15*8?"
path = "Let me solve this step by step"
candidates = [
    "I'll calculate this multiplication",
    "Let me work through this problem",
    "I need to multiply 15 by 8"
]

# Get vote score only
from scorer.scoring import get_vote_score_simple
vote_score = get_vote_score_simple(question, path, candidates)
print(f"Vote score: {vote_score:.3f}")

# Get combined score (confidence + vote)
combined_score = get_overall_answer_score(question, path, candidate_paths=candidates)
print(f"Combined score: {combined_score:.3f}")

# For detailed vote information, use AnswerScorer directly
from scorer.scoring import AnswerScorer
scorer = AnswerScorer()
vote_details = scorer.get_vote_score(question, path, candidates, include_raw=True)
print(f"Merge count: {vote_details.merge_count}")
print(f"Cluster sizes: {vote_details.cluster_sizes}")
print(f"Clustering method: {vote_details.raw_response['clustering_method']}")
```
