# Answer Confidence Scoring System

This directory contains a comprehensive scoring system for evaluating the confidence of answers generated by Large Language Models (LLMs).

## 📁 Files Overview

- **`scoring.py`** - Main scoring system with confidence calculation methods
- **`demo.py`** - Interactive demonstration and testing script

## 🎯 What is Confidence Scoring?

When an AI model generates text, it doesn't just pick one option - it considers many possible words/tokens and assigns probabilities to each. **Confidence scoring** uses these probabilities to determine how "sure" the model is about its answer.

Think of it like asking someone a math question:
- High confidence: "2+2=4" (very sure)
- Low confidence: "What's the 47th digit of pi? Um... maybe 3?" (uncertain)

## 🧮 Confidence Scoring Methods Explained

### 1. **Average Confidence** (Recommended)
- **What it is**: The average confidence across all tokens in the answer
- **Scale**: 0.0 to 1.0 (higher = more confident)
- **Use when**: You want a balanced overall confidence score
- **Example**: If the model generates "The answer is 4" with token confidences [0.9, 0.8, 0.7, 0.9], the average is 0.825

### 2. **Minimum Confidence** (Conservative)
- **What it is**: The confidence of the least confident token
- **Scale**: 0.0 to 1.0 (higher = more confident)
- **Use when**: You want to identify weak spots in the answer
- **Example**: Same answer as above, minimum confidence would be 0.7 (the weakest link)

### 3. **Geometric Confidence** (Multiplicative)
- **What it is**: Geometric mean of all token confidences
- **Scale**: 0.0 to 1.0 (higher = more confident)
- **Use when**: You want to penalize low-confidence tokens more heavily
- **Example**: More sensitive to outliers than average confidence

### 4. **Perplexity** (Technical)
- **What it is**: A measure of how "surprised" the model is by its own answer
- **Scale**: 1.0+ (lower = more confident)
- **Use when**: You're doing technical analysis or comparing across different text lengths
- **Example**: Perplexity of 1.2 (good) vs 5.8 (poor)

## 🏆 Overall Scoring Methods

The scoring system provides three different ways to get an overall score that combines multiple factors (currently just confidence, but designed for future expansion):

### 1. `get_overall_score()` - 📊 Detailed Analysis
- **Type**: Class method (requires `AnswerScorer` instance)
- **Returns**: Large dictionary with complete breakdown
- **Best for**: Debugging, analysis, understanding what's happening
- **Performance**: Slower (more data processing)

```python
scorer = AnswerScorer()
result = scorer.get_overall_score("What is 2+2?", "")

# Returns detailed dictionary:
{
    'overall_score': 0.756,
    'component_scores': {
        'confidence': {'score': 0.756, 'details': {...}},
        'parent_child_quality': {'score': 0.0, 'details': {...}},
        # ... more components
    },
    'weights_used': {'confidence': 1.0},
    'total_weight': 1.0,
    'metadata': {...}
}
```

### 2. `get_simple_overall_score()` - 🔢 Just the Number  
- **Type**: Class method (requires `AnswerScorer` instance)
- **Returns**: Single float (0-1)
- **Best for**: When you already have a scorer instance but just need the final score
- **Performance**: Fast

```python
scorer = AnswerScorer()
score = scorer.get_simple_overall_score("What is 2+2?", "")
# Returns: 0.756
```

### 3. `get_overall_answer_score()` - ⚡ One-Liner
- **Type**: Standalone function (no class needed)
- **Returns**: Single float (0-1)
- **Best for**: Search algorithms, quick scoring, when you don't want to create instances
- **Performance**: Fast

```python
from scorer.scoring import get_overall_answer_score

score = get_overall_answer_score("What is 2+2?", "")
# Returns: 0.756
```

### 🎯 Which One Should You Use?

| Use Case | Recommended Method | Why? |
|----------|-------------------|------|
| **Search Algorithms** | `get_overall_answer_score()` | Simple one-liner, no instance needed |
| **Tree Search (Beam, MCTS)** | `get_overall_answer_score()` | Fast, clean integration |
| **Debugging/Analysis** | `get_overall_score()` | See all component scores and details |
| **Batch Processing** | `get_overall_answer_score()` | Stateless, easy to parallelize |
| **Research/Tuning** | `get_overall_score()` | Full visibility into scoring components |

### 🔗 Method Relationship
```
get_overall_answer_score()  
    ↓ creates AnswerScorer instance
    ↓ calls get_simple_overall_score()
        ↓ calls get_overall_score()
        ↓ extracts just the 'overall_score' field
```

## 🚀 Quick Start

### Prerequisites
Make sure your vLLM policy server is running:
```bash
python3 -m vllm.entrypoints.openai.api_server \
  --model xmu-nlp/Llama-3-8b-gsm8k \
  --port 8000 \
  --dtype float16 \
  --tensor-parallel-size 2 \
  --swap-space 8 \
  --max-model-len 4096
```

### Run the Demo
```bash
cd /workspace/Fetch
python scorer/demo.py
```

This will show you:
- ✅ Basic confidence scoring examples
- 📊 Detailed scoring with all metrics
- 🪜 Step-by-step reasoning confidence
- 🔄 Comparison of different scoring methods
- 🏆 Overall scoring demonstrations

## 💻 Usage Examples

### For Search Algorithms (Recommended)
```python
from scorer.scoring import get_overall_answer_score

# In your beam search, MCTS, etc.
score = get_overall_answer_score("What is 2+2?", "")
print(f"Score: {score:.3f}")  # Score: 0.756

# With custom weights (when more components are available)
weights = {"confidence": 0.8, "length_penalty": 0.2}
score = get_overall_answer_score("What is 2+2?", "", weights=weights)
```

### For Analysis and Debugging
```python
from scorer.scoring import AnswerScorer

scorer = AnswerScorer()
result = scorer.get_overall_score("What is 15*23?", "")

print(f"Overall Score: {result['overall_score']:.3f}")
print(f"Confidence: {result['component_scores']['confidence']['score']:.3f}")
print(f"Generated Text: {result['component_scores']['confidence']['details']['text_generated']}")
```

### Simple Confidence Check
```python
from scorer.scoring import get_simple_confidence

# Get just confidence score (0-1)
confidence = get_simple_confidence("What is 2+2?")
print(f"Confidence: {confidence:.3f}")
```

### Step-by-Step Reasoning
```python
# Evaluate confidence with previous context
question = "What is 25*16?"
context = "I need to multiply 25 by 16. Let me break this down:"

score = get_overall_answer_score(question, context)
print(f"With context score: {score:.3f}")
```

### Batch Processing
```python
questions = ["What is 1+1?", "What is 5*7?", "What is 100/4?"]
scores = [get_overall_answer_score(q, "") for q in questions]
print(f"Scores: {[f'{s:.3f}' for s in scores]}")
```

## 🔧 Configuration

You can customize the scoring system:

```python
from scorer.scoring import PolicyConfig, AnswerScorer

# Custom configuration
config = PolicyConfig(
    url="http://127.0.0.1",
    port=8000,
    model_name="your-model-name",
    temperature=0.5,
    max_tokens=256
)

scorer = AnswerScorer(config)
```

## 📊 Understanding Results

### Good Overall Scores
- **Score > 0.7**: Model is quite confident overall
- **Score > 0.5**: Acceptable confidence  
- **Score > 0.3**: Lower confidence but potentially usable

### Poor Overall Scores
- **Score < 0.3**: Model is uncertain
- **Score < 0.1**: Very low confidence, likely unreliable

### Example Output
```
📝 Question: What is 12*15?
🏆 Overall Score: 0.756

📊 Component Breakdown:
   • confidence: 0.756
   • length_penalty: 1.000

🔤 Token Details:
   'The' → 0.834
   ' answer' → 0.756
   ' is' → 0.623
   ' 180' → 0.812
   '.' → 0.891
```

## 🔍 Troubleshooting

### "Failed to parse" URL Error
**Problem**: URL shows as `http://127.0.0.1:8000:8000/v1/completions`
**Solution**: In `scoring.py`, line 38, change:
```python
# From:
url: str = "http://127.0.0.1:8000"
# To:
url: str = "http://127.0.0.1"
```

### "Cannot connect to server"
**Problem**: Demo shows connection error
**Solutions**:
1. Make sure vLLM server is running (see Prerequisites above)
2. Check if port 8000 is available
3. Verify the model name matches your server configuration

### "No confidence scores returned"
**Problem**: All confidence values are None or 0.0
**Solutions**:
1. Check that `logprobs=5` is being sent in the API request
2. Verify your vLLM server supports logprobs
3. Check server logs for errors

## 🧪 Testing Your Setup

Quick test to verify everything works:

```bash
cd /workspace/Fetch
python3 -c "
from scorer.scoring import get_overall_answer_score
try:
    score = get_overall_answer_score('What is 1+1?')
    print(f'✅ Success! Overall Score: {score:.3f}')
except Exception as e:
    print(f'❌ Error: {e}')
"
```

## 🔮 Future Features

The scoring system is designed to be extensible. Planned features include:

- **Parent/Child Node Quality**: Score based on reasoning tree structure
- **Semantic Similarity**: Compare answers to reference solutions
- **Coherence Scoring**: Evaluate logical flow and consistency
- **Factual Consistency**: Check against knowledge bases
- **Length Penalty**: Already implemented, can be enabled with weights
- **Custom Scoring Functions**: Add your own confidence metrics

## 📚 Integration Examples

### Beam Search Integration
```python
from scorer.scoring import get_overall_answer_score

def evaluate_beam_candidates(question, candidates):
    scored_candidates = []
    for candidate_path in candidates:
        score = get_overall_answer_score(question, candidate_path)
        scored_candidates.append((candidate_path, score))
    return sorted(scored_candidates, key=lambda x: x[1], reverse=True)
```

### MCTS Integration
```python
from scorer.scoring import get_overall_answer_score

class MCTSNode:
    def __init__(self, question, path):
        self.question = question
        self.path = path
        self.confidence_score = get_overall_answer_score(question, path)
    
    def get_value(self):
        # Combine verifier score with confidence
        return self.verifier_value * 0.7 + self.confidence_score * 0.3
```

---

*Need help? Check the demo output or create an issue with your specific error message.*
